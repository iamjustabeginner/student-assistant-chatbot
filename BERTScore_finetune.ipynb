{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from bert_score import score\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: scraped_data_1717508117.md not found in markdown folder\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file containing the generated answers from the fine-tuned model\n",
    "finetuned_answers = pd.read_csv('/home/subin/Desktop/subin/finetuned_model_answers_2.csv')  # Adjust path if needed\n",
    "\n",
    "# Function to extract text from Markdown files\n",
    "def extract_text_from_markdown(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        md_content = file.read()\n",
    "    # Convert Markdown to HTML\n",
    "    html_content = markdown.markdown(md_content)\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Extract and return plain text\n",
    "    return soup.get_text()\n",
    "\n",
    "# Load and extract text from all Markdown files in the reference folder\n",
    "markdown_folder = '/home/subin/Desktop/subin/ritsu_bot/markdown_files'\n",
    "all_reference_texts = {}\n",
    "for file_path in glob.glob(os.path.join(markdown_folder, '*.md')):\n",
    "    filename = os.path.basename(file_path)\n",
    "    all_reference_texts[filename] = extract_text_from_markdown(file_path).strip()\n",
    "\n",
    "# Extract reference texts and generated answers for BERTScore\n",
    "references = []\n",
    "hypotheses = finetuned_answers['Answer'].str.strip().tolist()\n",
    "\n",
    "for index, row in finetuned_answers.iterrows():\n",
    "    filename = row['Filename']\n",
    "    if filename in all_reference_texts:\n",
    "        reference_text = all_reference_texts[filename]\n",
    "    else:\n",
    "        print(f\"Warning: {filename} not found in markdown folder\")\n",
    "        reference_text = \"\"  # Assign empty text or handle as needed\n",
    "    references.append(reference_text)\n",
    "\n",
    "# Filter out empty references and corresponding hypotheses\n",
    "filtered_references = [ref for ref in references if ref]\n",
    "filtered_hypotheses = [hyp for ref, hyp in zip(references, hypotheses) if ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b6a5a5950c437d922e8a0437e49e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c4ec4417784cdbb0887dee44f7ac45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 9.61 seconds, 20.30 sentences/sec\n",
      "Average BERTScore F1: 0.827528178691864\n",
      "BERTScore results saved to /home/subin/Desktop/subin/BERTScore_finetuned.csv\n"
     ]
    }
   ],
   "source": [
    "# Calculate BERTScore\n",
    "P, R, F1 = score(filtered_hypotheses, filtered_references, lang=\"en\", verbose=True)\n",
    "\n",
    "# Average F1 score\n",
    "average_f1 = F1.mean().item()\n",
    "print(f\"Average BERTScore F1: {average_f1}\")\n",
    "\n",
    "# Add the BERT F1 scores to the DataFrame\n",
    "finetuned_answers['BERTScore F1'] = F1.tolist() + [None] * (len(hypotheses) - len(F1))\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_csv_path = '/home/subin/Desktop/subin/BERTScore_finetuned.csv'\n",
    "finetuned_answers.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"BERTScore results saved to {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
