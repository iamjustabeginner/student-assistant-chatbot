{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "import langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple set of stopwords (commonly used words that don't add much meaning)\n",
    "STOPWORDS = {\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n",
    "    \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"it\", \"its\", \"itself\", \"they\", \n",
    "    \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \n",
    "    \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \n",
    "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \n",
    "    \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \n",
    "    \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \n",
    "    \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \n",
    "    \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \n",
    "    \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \n",
    "    \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \n",
    "    \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \n",
    "    \"should\", \"now\"\n",
    "}\n",
    "\n",
    "def count_words_and_sentences(content):\n",
    "    # Word tokenization: split by whitespace and strip punctuation\n",
    "    words = re.findall(r'\\b\\w+\\b', content.lower())  # Get words using regex\n",
    "    # Sentence tokenization: split by common sentence-ending punctuation\n",
    "    sentences = re.split(r'[.!?]+', content)  # Splits on ., !, ?\n",
    "    return words, [s for s in sentences if s.strip()]\n",
    "\n",
    "def get_language_distribution(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except langdetect.lang_detect_exception.LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "def count_words_in_directory(directory_path):\n",
    "    total_words = []\n",
    "    total_sentences = 0\n",
    "    file_count = 0\n",
    "    language_counter = Counter()\n",
    "\n",
    "    # Get all markdown files in the directory\n",
    "    markdown_files = glob.glob(os.path.join(directory_path, '**/*.md'), recursive=True)\n",
    "    file_count = len(markdown_files)\n",
    "\n",
    "    for md_file in markdown_files:\n",
    "        with open(md_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            words, sentences = count_words_and_sentences(content)\n",
    "            total_words.extend(words)\n",
    "            total_sentences += len(sentences)\n",
    "\n",
    "            # Detect language\n",
    "            language = get_language_distribution(content)\n",
    "            language_counter[language] += 1\n",
    "\n",
    "    return total_words, total_sentences, file_count, language_counter\n",
    "\n",
    "def get_file_size_in_mb(directory_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "def get_top_frequent_words(words, top_n=10):\n",
    "    word_count = Counter(words)\n",
    "    return word_count.most_common(top_n)\n",
    "\n",
    "def get_word_frequency_distribution(words):\n",
    "    word_count = Counter(words)\n",
    "    return word_count\n",
    "\n",
    "def stopword_usage(words):\n",
    "    stopword_count = sum(1 for word in words if word in STOPWORDS)\n",
    "    return stopword_count, len(STOPWORDS)\n",
    "\n",
    "# Specify the directory where your markdown files are stored\n",
    "directory_path = '/home/subin/Desktop/subin/ritsu_bot/markdown_files'\n",
    "\n",
    "# Collecting all statistics\n",
    "all_words, total_sentences, file_count, language_counter = count_words_in_directory(directory_path)\n",
    "total_word_count = len(all_words)\n",
    "average_word_count = total_word_count / file_count if file_count > 0 else 0\n",
    "total_file_size_mb = get_file_size_in_mb(directory_path)\n",
    "unique_word_count = len(set(all_words))\n",
    "stopword_count, total_stopwords = stopword_usage(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of document files: 196\n",
      "Total word count: 196835\n",
      "Total sentence count: 10387\n",
      "Average word count per document: 1004.26\n",
      "Total storage size of dataset: 1.26 MB\n",
      "Unique words: 8126\n",
      "Top 10 most frequent words: [('td', 19134), ('1', 11276), ('colspan', 9902), ('the', 6046), ('of', 3787), ('and', 3489), ('to', 3353), ('in', 2719), ('valign', 2718), ('tr', 2312)]\n",
      "Language distribution: Counter({'en': 196})\n",
      "Stopword usage: 43544 stopwords used out of 124 available stopwords\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 most frequent words\n",
    "top_10_words = get_top_frequent_words(all_words, top_n=10)\n",
    "word_freq_distribution = get_word_frequency_distribution(all_words)\n",
    "\n",
    "# Print results\n",
    "print(f\"Total number of document files: {file_count}\")\n",
    "print(f\"Total word count: {total_word_count}\")\n",
    "print(f\"Total sentence count: {total_sentences}\")\n",
    "print(f\"Average word count per document: {average_word_count:.2f}\")\n",
    "print(f\"Total storage size of dataset: {total_file_size_mb:.2f} MB\")\n",
    "print(f\"Unique words: {unique_word_count}\")\n",
    "print(f\"Top 10 most frequent words: {top_10_words}\")\n",
    "print(f\"Language distribution: {language_counter}\")\n",
    "print(f\"Stopword usage: {stopword_count} stopwords used out of {total_stopwords} available stopwords\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
