{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your markdown files\n",
    "markdown_dir = \"/home/subin/Desktop/subin/ritsu_bot/markdown_files\"  # Replace with your markdown directory\n",
    "\n",
    "# Function to load markdown content\n",
    "def load_markdown_content(filename):\n",
    "    filepath = os.path.join(markdown_dir, filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth answers\n",
    "def get_ground_truth(filename, question):\n",
    "    original_text = load_markdown_content(filename)\n",
    "    # TODO: Implement a better answer extraction logic here if needed\n",
    "    return original_text  # For now, return the entire markdown content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3045\n",
      "Recall: 0.1769\n",
      "F1 Score: 0.1964\n"
     ]
    }
   ],
   "source": [
    "# CSV file containing Question and Generated_Answer\n",
    "csv_path = \"/home/subin/Desktop/subin/finetuned_model_answers_2.csv\"  # Replace with your CSV file path\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Add a column for ground truth answers\n",
    "data['Ground_Truth'] = data.apply(\n",
    "    lambda row: get_ground_truth(row['Filename'], row['Question']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Tokenize the answers (simple whitespace splitting for this example)\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Compute Precision, Recall, and F1\n",
    "def compute_metrics(data):\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1s = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        # Tokenize ground truth and generated answers\n",
    "        ground_truth_tokens = set(tokenize(str(row['Ground_Truth'])))\n",
    "        generated_tokens = set(tokenize(str(row['Answer'])))\n",
    "\n",
    "        # Calculate true positives, false positives, and false negatives\n",
    "        true_positives = len(ground_truth_tokens & generated_tokens)\n",
    "        false_positives = len(generated_tokens - ground_truth_tokens)\n",
    "        false_negatives = len(ground_truth_tokens - generated_tokens)\n",
    "\n",
    "        # Precision, Recall, and F1\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1s.append(f1)\n",
    "\n",
    "    # Average metrics across all examples\n",
    "    avg_precision = sum(all_precisions) / len(all_precisions)\n",
    "    avg_recall = sum(all_recalls) / len(all_recalls)\n",
    "    avg_f1 = sum(all_f1s) / len(all_f1s)\n",
    "\n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Calculate metrics\n",
    "precision, recall, f1 = compute_metrics(data)\n",
    "\n",
    "# Print results\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
